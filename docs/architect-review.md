# Arvak — Architect Review (2026-02-16)

Generated by architect-mcp. Database: /Users/danielhinderink/Projects/architect-mcp/data/architect.db

## Technical Debt Register

### CRITICAL

**#9 [security] No request body size limits on gRPC server**
Axum/tonic server accepts arbitrarily large circuit payloads. A malicious or buggy client could send a 1GB circuit definition, causing OOM on the server.
- **Triggers when:** Any untrusted client access (public API, shared HPC deployment)
- **Blast radius:** Server OOM crash, all in-flight jobs lost, service restart required

**#8 [scaling] SQLite storage uses single blocking Mutex**
SqliteStorage wraps Connection in Arc<Mutex<Connection>> (std::sync::Mutex, not tokio). All database operations serialize through one lock. Called from async context via spawn_blocking(), but still a single-writer bottleneck.
- **Triggers when:** >50 concurrent job submissions/sec with SQLite backend
- **Blast radius:** Job submission latency spikes, queue buildup, timeouts cascade to clients. Dashboard queries block behind write lock.

**#7 [architecture] Job cancellation cannot interrupt running backend**
spawn_job_execution() spawns a tokio task but does not store the JoinHandle/AbortHandle. cancel_job() only updates the status in the store — it cannot abort a backend.submit() or backend.wait() that is already in flight. On real QPU hardware where jobs take minutes, cancelled jobs will continue consuming backend resources.
- **Triggers when:** First real QPU backend with jobs >30s execution time
- **Blast radius:** Wasted QPU time (billed), resource starvation on backend, user sees cancelled but backend still running

### HIGH

**#13 [integration] No retry logic for backend failures**
backend.submit() failures are immediately terminal — job moves to Failed state. No exponential backoff, no transient error detection, no circuit resubmission. Transient QPU errors (queue full, calibration in progress, network blip) kill the job permanently.
- **Triggers when:** First real QPU backend with transient failures (all real hardware has these)
- **Blast radius:** Unnecessary job failures, user must manually resubmit, poor UX for enterprise customers

**#12 [scaling] Rate limiter uses unbounded Vec per client**
RateLimitState.requests is a Vec<Instant> that grows with each request within the time window. At 1000 RPS per client with a 1-second window, this accumulates 1000+ Instant entries per client per cycle. No pruning of expired entries between checks.
- **Triggers when:** >100 RPS from a single client IP with rate limiting enabled
- **Blast radius:** Memory growth proportional to request rate, GC pressure, slower rate limit checks as Vec grows

**#11 [operability] Hardcoded /tmp path for scheduler state**
Scheduler persists state to /tmp/arvak-scheduler. On shared HPC nodes (LUMI, LRZ), multiple users could collide. Path is not configurable.
- **Triggers when:** Multi-user HPC deployment (the primary target environment)
- **Blast radius:** Scheduler state corruption between users, job routing failures, silent data overwrites

**#10 [security] No authentication on gRPC or dashboard**
Neither the gRPC service nor the web dashboard implement authentication. Code comments say "deploy behind nginx or Envoy with mTLS" but no enforcement exists. Any network-reachable client can submit jobs, query results, and access the dashboard.
- **Triggers when:** Any deployment accessible beyond localhost (HPC cluster, cloud, demo)
- **Blast radius:** Unauthorized job submission consuming compute resources, data exfiltration of job results, dashboard manipulation

### MEDIUM

**#19 [operability] Python bindings have no shot count validation**
run_sim(circuit, shots) accepts any u32 for shots. Python caller could pass shots=1_000_000_000, causing the statevector simulator to allocate massive result arrays.
- **Triggers when:** Inexperienced Python user or fuzzing
- **Blast radius:** OOM in Rust runtime, Python process crashes without useful error

**#18 [data] In-memory storage has no eviction policy**
MemoryStorage keeps all jobs in FxHashMap forever. No TTL, no LRU eviction, no max capacity. Long-running server accumulates job history until OOM.
- **Triggers when:** >100K jobs submitted without restart (days of continuous operation)
- **Blast radius:** Gradual memory growth, eventual OOM, all in-flight jobs lost on crash

**#17 [testing] No integration tests for real QPU backend adapters**
IQM, IBM, and CUDA-Q adapters have unit tests with mocks but no integration tests against real hardware. The DDSIM integration test exists in nightly CI but only covers the simulator path.
- **Triggers when:** First deployment against real QPU hardware
- **Blast radius:** Unknown failure modes in auth, circuit translation, result parsing, timeout handling on real hardware

**#16 [security] Dashboard CORS defaults to permissive**
ARVAK_CORS_ORIGIN defaults to permissive if not set. Any origin can make requests to the dashboard API. Combined with no auth, this means any webpage can query job results.
- **Triggers when:** Dashboard exposed to network without explicit CORS config
- **Blast radius:** Cross-origin data access, job result exfiltration via browser

**#15 [scaling] Circuit cloned in job submission hot path**
Job submission parses the circuit, then clones it for the async execution task (moved into tokio::spawn). For large circuits (1000+ gates), this clone is expensive — allocating a full DAG copy on every submission.
- **Triggers when:** >100 jobs/sec with circuits >500 gates
- **Blast radius:** Increased submission latency, memory pressure, GC spikes

**#20 [build] arvak-grpc unconditionally pulls heavy optional dependencies**
`arvak-grpc` unconditionally depends on opentelemetry, prometheus, axum, tower-http, and tonic-reflection. Downstream consumers like `arvak-edge` that only need `ArvakServiceImpl` still compile all of these. Binary ~30MB vs ideal ~15MB.
- **Triggers when:** Building slim binaries that reuse `arvak-grpc` service logic (e.g., `arvak-edge`)
- **Blast radius:** Unnecessary compile time and binary bloat for edge/embedded deployments
- **Fix:** Feature-gate `otel`, `http-health`, and `reflection` in `arvak-grpc/Cargo.toml`

**#14 [operability] OpenTelemetry 10 versions behind (0.21 vs 0.31)**
opentelemetry dependency at 0.21, current is 0.31. Missing performance improvements, bug fixes, and new trace/metric features. Also affects compatibility with modern collectors.
- **Triggers when:** Integration with modern observability stack (Grafana, Datadog, etc.)
- **Blast radius:** Incompatible trace formats, missing features, potential security patches missed

**#6 [operability] No runtime error diagnostics for async gRPC failures**
Rust async/tokio/tonic stack produces opaque error messages on runtime failures. No structured error hierarchy, no error codes, no troubleshooting guide.
- **Triggers when:** First production gRPC timeout or connection failure
- **Blast radius:** Non-developer founder cannot diagnose or communicate the issue. Requires deep Rust async knowledge to debug.

## Architectural Decisions

### Fire-and-forget job execution (spawn per job)
Each job submission spawns a new tokio task. The task runs independently — no pool, no queue, no backpressure beyond ResourceManager limits.
- **Rationale:** Simple model. Tokio handles thousands of lightweight tasks efficiently. ResourceManager provides per-client limits. For a compiler service where most jobs complete in <1s, this is appropriate.
- **Alternatives considered:** Job queue with worker pool: more predictable resource usage but added complexity. Channel-based pipeline: better backpressure but harder to implement streaming.
- **Trade-offs:** No backpressure at task level (ResourceManager checks at submission, not execution). No task reuse. JoinHandle not stored — jobs cannot be truly cancelled.
- **Revisit when:** If job execution regularly takes >30s (real QPU), or if >1000 concurrent jobs cause tokio scheduler pressure, or if job cancellation becomes a customer requirement.

### No authentication at application layer
gRPC server and dashboard have no built-in auth. Intended to deploy behind reverse proxy (nginx/Envoy) with mTLS.
- **Rationale:** Keeps Arvak focused on compilation and execution. Auth is a cross-cutting concern better handled at infrastructure layer. Avoids reimplementing token management, session handling, etc.
- **Alternatives considered:** Built-in JWT auth: added complexity, token management, key rotation. API key auth: simpler but still application-level concern.
- **Trade-offs:** ANY deployment without a proxy is completely open. Developer convenience (no auth in dev) at the cost of production safety. Easy to forget the proxy requirement.
- **Revisit when:** First production deployment, or if Arvak is ever exposed directly to the internet (e.g., SaaS model). At that point, at minimum add API key auth as a compile-time feature.

### HAL Contract v2 — sync capabilities, async execution
Backend trait split: capabilities(), is_available(), validate() are synchronous. execute() is async. This means capability queries never block, but execution can take arbitrarily long.
- **Rationale:** Quantum backends have static capabilities (gate set, qubit count) but variable execution time. Sync capabilities enable fast routing decisions without I/O. Async execution handles QPU queue times.
- **Alternatives considered:** Fully async (everything returns Future): simpler interface but forces async overhead on capability queries. Fully sync: impossible for real QPU execution.
- **Trade-offs:** Mixed sync/async is slightly unusual. Implementers must understand the distinction. Capabilities cached at init time — if hardware recalibrates, cached data may be stale.
- **Revisit when:** If backends need dynamic capabilities (e.g., qubit count changes after recalibration) or if the sync capability assumption proves wrong for a specific hardware platform.

### Pluggable storage behind trait (memory/SQLite/Postgres)
JobStorage trait with three implementations. Memory for development/testing, SQLite for single-node production, PostgreSQL for multi-node production.
- **Rationale:** Allows zero-dependency development (memory), simple deployment (SQLite), and scalable production (Postgres) without code changes. Feature-gated at compile time.
- **Alternatives considered:** Single storage backend (Postgres only): simpler but requires DB for development. Redis: good for jobs but poor for result storage.
- **Trade-offs:** Three implementations to maintain. SQLite has single-writer bottleneck. Memory has no persistence. Postgres requires infrastructure.
- **Revisit when:** If a fourth storage backend is needed, or if the trait interface proves too limiting for advanced queries (pagination, full-text search on circuit metadata).

## Recommended Fix Priority

### Before any deployment beyond localhost
1. Add gRPC max message size (request body limits) — ~10 lines of config
2. Add auth or document proxy requirement as hard gate
3. Fix /tmp scheduler path to use config or XDG_RUNTIME_DIR

### Before first real QPU backend
4. Store JoinHandle for job cancellation (AbortHandle pattern)
5. Add retry logic with exponential backoff for transient backend failures
6. Add integration test scaffold for real hardware (even if mocked initially)

### Before first enterprise customer
7. Switch to PostgreSQL storage backend (SQLite Mutex bottleneck)
8. Fix rate limiter Vec to ring buffer
9. Update OpenTelemetry to current version
10. Add shot count validation in Python bindings
11. Add memory eviction policy to MemoryStorage
12. Set explicit CORS origin (not permissive default)